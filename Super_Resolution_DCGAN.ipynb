{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Super_Resolution_DCGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3182bea4e8f4034a0c6b4b0b81c95dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_be51630572b3459bb4c7ad064863148b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fb2cdfc03a9847f59e5aa1f889ad1680",
              "IPY_MODEL_8c356152d2a149e3b120d055729302c2"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "be51630572b3459bb4c7ad064863148b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "fb2cdfc03a9847f59e5aa1f889ad1680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_560edbb5daed46388eaed321fd4128b0",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 202599,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 202599,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_578002e559bb430e8171d151738400cb"
          },
          "model_module_version": "1.5.0"
        },
        "8c356152d2a149e3b120d055729302c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_42671a3002be4a4ba7f08780569596b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 202599/202599 [09:28&lt;00:00, 356.65it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_88a745caac714d2bb33a2672384a3c5d"
          },
          "model_module_version": "1.5.0"
        },
        "560edbb5daed46388eaed321fd4128b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "578002e559bb430e8171d151738400cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "42671a3002be4a4ba7f08780569596b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "88a745caac714d2bb33a2672384a3c5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNvDCd6a7CnV"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import PIL\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tqdm.notebook as tq\n",
        "import os\n",
        "from PIL import Image\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import initializers\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.activations import relu\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import UpSampling2D\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "import gc\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Q_xiGHFkWI"
      },
      "source": [
        "!pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMXTgEuG1lD6"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3WzkIV3Dj4j"
      },
      "source": [
        "### Importing the dataset from Kaggle: The CelebA Dataset consisting of about 200,000 images of people.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEO8HKWGFtZY",
        "outputId": "1c66d70b-6631-44f7-d3cd-0276ab6c6d18"
      },
      "source": [
        "!kaggle datasets list -s celeba"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
            "ref                                                   title                                        size  lastUpdated          downloadCount  \n",
            "----------------------------------------------------  ------------------------------------------  -----  -------------------  -------------  \n",
            "jessicali9530/celeba-dataset                          CelebFaces Attributes (CelebA) Dataset        1GB  2018-06-01 20:08:48          43119  \n",
            "zuozhaorui/celeba                                     celeba                                        3GB  2018-11-03 05:29:21            342  \n",
            "ashishjangra27/gender-recognition-200k-images-celeba  Gender Classification 200K Images | CelebA    1GB  2020-05-22 20:15:23            319  \n",
            "ruchi798/periocular-detection                         Periocular Recognition                       13MB  2020-08-09 00:45:00            134  \n",
            "ahmedshawaf/celeba                                    celeba                                        1GB  2020-04-07 21:56:35            133  \n",
            "megh24/celeba-tfrecs                                  CelebA TFRecords                             11GB  2020-04-13 09:38:53             38  \n",
            "tannys26/face-extracted-from-celeba-dataset           Celebrity Face Dataset                      810MB  2020-05-29 04:37:05             43  \n",
            "shaswatshubham/celeba-tfrecord                        Celeba Tfrecord                             529MB  2020-09-19 18:12:27              6  \n",
            "aniruddhakalkar/celeba-alligned                       CelebA Alligned                               3GB  2018-04-16 14:33:41            108  \n",
            "janluke/celeba-aligned                                CelebA Aligned                                1GB  2019-03-24 14:33:41              0  \n",
            "kevinpatel04/celeba-original-wild-images              CelebA - Original Wild Images                17GB  2020-04-05 12:59:51             96  \n",
            "ashishjangra27/gender-detection-20k-images-celeba     Gender Classification 20K Images | CelebA   134MB  2020-05-21 22:10:13            104  \n",
            "davidedig/celeba-redux                                celebA_redux                                209MB  2019-05-15 10:01:13             53  \n",
            "rkuo2000/celeba                                       celeba                                      312KB  2020-11-18 12:19:16              0  \n",
            "lamsimon/celebahq                                     celeba-hq                                     3GB  2020-07-28 14:46:35            156  \n",
            "megh24/msggan-tfrecs                                  TFrecords for CelebA                         11GB  2020-04-09 06:39:15             11  \n",
            "respocroissants/celeba-50-people-subset               CelebA 50 people subset                       9MB  2020-02-13 16:33:09              5  \n",
            "udaysaig/celeba                                       celebA                                        1GB  2020-07-01 16:20:36              0  \n",
            "sinyouchang/celeba                                    Celeba                                        1GB  2020-08-20 11:08:32              5  \n",
            "benjaminkz/celeba                                     CelebA                                        1GB  2020-06-06 00:50:03              4  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QXkFN0OGJjX",
        "outputId": "44591ae6-0d5f-4356-b402-21da32ad35eb"
      },
      "source": [
        "!kaggle datasets download -d jessicali9530/celeba-dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading celeba-dataset.zip to /content\n",
            "100% 1.32G/1.33G [00:23<00:00, 52.7MB/s]\n",
            "100% 1.33G/1.33G [00:23<00:00, 60.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8yRgwdXi3g-",
        "outputId": "a7363bb2-46ea-44cf-d47c-73ab567d4a54"
      },
      "source": [
        "!unzip '/content/celeba-dataset.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/celeba-dataset.zip\n",
            "replace img_align_celeba/img_align_celeba/000001.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98x2VLpauN4s"
      },
      "source": [
        "### Loading the data and preprocessing it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0I7itTW8oj3"
      },
      "source": [
        "os.mkdir('/content/reduced_dataset')\n",
        "os.mkdir('/content/reduced_dataset/reduced_dataset/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyzH82NnC2fF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "e3182bea4e8f4034a0c6b4b0b81c95dd",
            "be51630572b3459bb4c7ad064863148b",
            "fb2cdfc03a9847f59e5aa1f889ad1680",
            "8c356152d2a149e3b120d055729302c2",
            "560edbb5daed46388eaed321fd4128b0",
            "578002e559bb430e8171d151738400cb",
            "42671a3002be4a4ba7f08780569596b8",
            "88a745caac714d2bb33a2672384a3c5d"
          ]
        },
        "outputId": "58a36509-45f3-4775-fef8-2a77064fdbfc"
      },
      "source": [
        "directory = '/content/img_align_celeba/img_align_celeba/'\n",
        "original_ht = 208\n",
        "original_width = 178\n",
        "diff = (original_ht - original_width)//2\n",
        "for image in tq.tqdm(os.listdir(directory)):\n",
        "    img = Image.open(directory + image)\n",
        "    img = img.crop((0, diff, original_width, original_ht-diff))\n",
        "    img.thumbnail((128, 128), Image.ANTIALIAS)\n",
        "    img.save(\"reduced_dataset/reduced_dataset/\" + image)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3182bea4e8f4034a0c6b4b0b81c95dd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=202599.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-QCyLG_vgrD"
      },
      "source": [
        "### Using ImageDataGenerator of Keras to load the large dataset into batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq4BPJ4IC2cl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7360cdf6-1c5a-4b80-f985-da580185883b"
      },
      "source": [
        "def preprocessing_function(x):\n",
        "    return x/128. - 1.\n",
        "\n",
        "datagen = ImageDataGenerator(preprocessing_function=preprocessing_function, validation_split=0.1)\n",
        "\n",
        "train_ds = datagen.flow_from_directory('reduced_dataset/',\n",
        "                                             target_size=(128, 128), batch_size=128,\n",
        "                                             class_mode=None, subset='training')\n",
        "valid_ds = datagen.flow_from_directory('reduced_dataset/',\n",
        "                                             target_size=(128, 128), batch_size=128,\n",
        "                                             class_mode=None, subset='validation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 182340 images belonging to 1 classes.\n",
            "Found 20259 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4j71YjkwHyi"
      },
      "source": [
        "### Building the Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX8sn9b97QWf"
      },
      "source": [
        "class ResidualUnit(keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, strides=1, activation=LeakyReLU(alpha=0.2), **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.main_layers = [Conv2D(filters, kernel_size=3, strides=strides, padding=\"SAME\"),\n",
        "                            BatchNormalization(),\n",
        "                            self.activation,\n",
        "                            Conv2D(filters, kernel_size=3, strides=strides, padding=\"SAME\"),\n",
        "                            BatchNormalization(),\n",
        "        ]\n",
        "        self.skip_layers = [Conv2D(filters, kernel_size=1, strides=strides, padding=\"SAME\"),\n",
        "                            BatchNormalization(),\n",
        "        ]\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.main_layers:\n",
        "            Z = layer(Z)\n",
        "        skip_Z = inputs\n",
        "        for layer in self.skip_layers:\n",
        "            skip_Z = layer(skip_Z)\n",
        "        return self.activation(Z + skip_Z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7r96EwN7R0Z"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "generator = Sequential()                                                   \n",
        "generator.add(Conv2D(64, kernel_size=7, strides=1, padding=\"SAME\",\n",
        "           activation=LeakyReLU(alpha=0.2), input_shape=[32, 32, 3]))\n",
        "\n",
        "for filters in [256, 128, 64]:\n",
        "    generator.add(ResidualUnit(filters, kernel_size=3, strides=1))\n",
        "    generator.add(ResidualUnit(filters, kernel_size=3, strides=1))\n",
        "\n",
        "generator.add(UpSampling2D(size=2))\n",
        "generator.add(Conv2D(64, kernel_size=3, strides=1, padding=\"SAME\"))\n",
        "generator.add(BatchNormalization())\n",
        "Activation(LeakyReLU(alpha=0.2))\n",
        "\n",
        "generator.add(UpSampling2D(size=2))\n",
        "generator.add(Conv2D(3, kernel_size=9, strides=1, padding=\"SAME\",\n",
        "           activation='tanh'))\n",
        "\n",
        "\n",
        "discriminator = Sequential()\n",
        "discriminator.add(Conv2D(64, kernel_size=3, strides=1, padding=\"SAME\",\n",
        "            activation=LeakyReLU(alpha=0.2), input_shape=[128, 128, 3]))\n",
        "\n",
        "for filters in [64, 128, 256, 512]:\n",
        "    discriminator.add(Conv2D(filters, kernel_size=3, strides=2, padding=\"SAME\"))\n",
        "    discriminator.add(BatchNormalization())\n",
        "    Activation(LeakyReLU(alpha=0.2))\n",
        "\n",
        "discriminator.add(Conv2D(256, kernel_size=3, strides=1, padding=\"SAME\"))\n",
        "discriminator.add(BatchNormalization())\n",
        "Activation(LeakyReLU(alpha=0.2))\n",
        "\n",
        "discriminator.add(Dense(1024))\n",
        "discriminator.add(BatchNormalization())\n",
        "discriminator.add(GlobalAveragePooling2D())\n",
        "discriminator.add(Dense(1, activation='sigmoid'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L3OZLIj7RxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d2d349-bfb6-4d3a-a5bd-5eab8f0e8f60"
      },
      "source": [
        "generator.summary()\n",
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 64)        9472      \n",
            "_________________________________________________________________\n",
            "residual_unit (ResidualUnit) (None, 32, 32, 256)       757504    \n",
            "_________________________________________________________________\n",
            "residual_unit_1 (ResidualUni (None, 32, 32, 256)       1249024   \n",
            "_________________________________________________________________\n",
            "residual_unit_2 (ResidualUni (None, 32, 32, 128)       477056    \n",
            "_________________________________________________________________\n",
            "residual_unit_3 (ResidualUni (None, 32, 32, 128)       313216    \n",
            "_________________________________________________________________\n",
            "residual_unit_4 (ResidualUni (None, 32, 32, 64)        119744    \n",
            "_________________________________________________________________\n",
            "residual_unit_5 (ResidualUni (None, 32, 32, 64)        78784     \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 64, 64, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 128, 128, 3)       15555     \n",
            "=================================================================\n",
            "Total params: 3,057,539\n",
            "Trainable params: 3,052,035\n",
            "Non-trainable params: 5,504\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_21 (Conv2D)           (None, 128, 128, 64)      1792      \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 64, 64, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 16, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 8, 8, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 8, 8, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 8, 8, 256)         1179904   \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 8, 8, 1024)        263168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 8, 8, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 3,040,961\n",
            "Trainable params: 3,036,481\n",
            "Non-trainable params: 4,480\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MVH3fW6wPms"
      },
      "source": [
        "### Some utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HUoQRT0SVAy"
      },
      "source": [
        "def plot_gan(SR_images, LR_images):\n",
        "    fig = plt.figure(figsize=(12,3))\n",
        "    for i in range(8):\n",
        "        plt.subplot(2, 8, i+1)\n",
        "        plt.imshow((SR_images[i] + 1 )*0.5)\n",
        "        plt.axis('off')\n",
        "        \n",
        "        plt.subplot(2, 8, i+8+1)\n",
        "        plt.imshow((LR_images[i] + 1 )*0.5)\n",
        "        plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQFPbINUHyya"
      },
      "source": [
        "def save_GIF_images(model, GIF_seed, epoch):\n",
        "    pred = model(GIF_seed, training = False)\n",
        "    fig = plt.figure(figsize=(12, 3))\n",
        "\n",
        "    for i in range(8):\n",
        "        plt.subplot(2, 8, i+1)\n",
        "        plt.imshow((pred[i] + 1 )*0.5)\n",
        "        plt.axis('off')\n",
        "    \n",
        "        plt.subplot(2, 8, i+8+1)\n",
        "        plt.imshow((GIF_seed[i] + 1 )*0.5)\n",
        "        plt.axis('off')\n",
        "    \n",
        "    plt.savefig('/content/drive/MyDrive/Colab/Super Resolution using DCGAN/GIF/image_at_epoch_{:04d}.png'.format(epoch + 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfYz7OnO7RuT"
      },
      "source": [
        "def mse_loss(HR_batch, SR_batch):\n",
        "    return tf.reduce_mean(tf.square(HR_batch - SR_batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFZQjD9f7Rrn"
      },
      "source": [
        "from keras.layers import Input\n",
        "\n",
        "discriminator_optimizer = keras.optimizers.RMSprop(lr=.0001, clipvalue=1.0, decay=1e-8)\n",
        "discriminator.compile(loss=\"binary_crossentropy\", optimizer=discriminator_optimizer)\n",
        "discriminator.trainable = False\n",
        "\n",
        "input = Input(shape=[32, 32, 3])\n",
        "SR_image = generator(input)\n",
        "gan_output = discriminator(SR_image)\n",
        "gan = Model(inputs=input, outputs=[SR_image, gan_output])\n",
        "generator_optimizer = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
        "gan.compile(loss=[mse_loss, \"binary_crossentropy\"], loss_weights=[0.8, 0.2],\n",
        "            optimizer=generator_optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK7yjQeaXWN_"
      },
      "source": [
        "checkpoint_dir = '/content/drive/MyDrive/Colab/Super Resolution using DCGAN/checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9Fd9j_u7Rol"
      },
      "source": [
        "def train_gan(generator, discriminator, dataset, batch_size, epochs):\n",
        "    GIF_seed = tf.image.resize(valid_ds[0], size=[32, 32])\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        G_loss_epoch = 0\n",
        "        D_loss_epoch = 0\n",
        "        print(\"Epoch no. \" + str(epoch + 1))\n",
        "        start_time = time.time()\n",
        "        \n",
        "        for batch_number in tq.tqdm(range(dataset.n//batch_size)):\n",
        "            HR_batch = dataset[batch_number]\n",
        "            LR_batch = tf.image.resize(HR_batch, size=[32, 32])\n",
        "            \n",
        "            #PHASE 1: train the Discriminator\n",
        "            SR_batch = generator(LR_batch)\n",
        "            random_wrong_labels = np.random.binomial(1, 0.05, size=[batch_size, 1])\n",
        "            \n",
        "            Y1 = tf.constant([[0.]]*batch_size)\n",
        "            Y1 += .1 * np.random.random_sample(Y1.shape)\n",
        "            discriminator.trainable = True\n",
        "            D_loss_epoch += discriminator.train_on_batch(SR_batch, Y1)\n",
        "            \n",
        "            Y1 = tf.constant([[1.]]*batch_size)\n",
        "            Y1 -= .1 * np.random.random_sample(Y1.shape)\n",
        "            discriminator.trainable = True\n",
        "            D_loss_epoch += discriminator.train_on_batch(HR_batch, Y1)\n",
        "                \n",
        "            \n",
        "            #PHASE 2: train the Generator\n",
        "            random_wrong_labels = np.random.binomial(1, 0.05, size=[batch_size, 1])\n",
        "            Y2 = tf.constant([[1.]]*batch_size)\n",
        "            Y2 -= .1 * np.random.random_sample(Y2.shape)\n",
        "            discriminator.trainable = False\n",
        "            G_losses = gan.train_on_batch(LR_batch, [HR_batch, Y2])\n",
        "            G_loss_epoch += G_losses[0]\n",
        "\n",
        "            if (batch_number + 1)%500 == 0:\n",
        "                plot_gan(SR_batch, LR_batch)\n",
        "                j = np.random.randint(0, 50)\n",
        "                plot_gan(generator(tf.image.resize(valid_ds[j], size=[32, 32])), tf.image.resize(valid_ds[j], size=[32, 32]))\n",
        "                checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "        D_loss_epoch /= (dataset.n//batch_size)\n",
        "        G_loss_epoch /= (dataset.n//batch_size)\n",
        "        print(\"D_loss = \"+str(D_loss_epoch)+\"   G_loss = \"+str(G_loss_epoch)+\"   @epoch \"+str(epoch+1)+\"   time = \"+str(time.time() - start_time))\n",
        "        plot_gan(SR_batch, LR_batch)\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"Saving weights and generating GIF images\")\n",
        "        save_GIF_images(generator, GIF_seed, epoch)\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "        \n",
        "    save_GIF_images(generator, GIF_seed, epoch)\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7X2N8rWyFSm"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HRK33396H8o"
      },
      "source": [
        "train_gan(generator, discriminator, train_ds, 128, epochs=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2PM7X31yI0o"
      },
      "source": [
        "### Results are in the accompanying Github README.md"
      ]
    }
  ]
}