{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxoSSrTHLc1J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49mXvvIQl_p7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Lambda\n",
        "from skimage import data, io, filters\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from numpy.random import randint\n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The dataset was downloaded and saved to the following path"
      ],
      "metadata": {
        "id": "0dR2ajULgnl7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoNRDRuQbwiH"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/Image_dataset_super_resolution_unziped/img_align_celeba/img_align_celeba/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzFjDejCMQJO"
      },
      "outputs": [],
      "source": [
        "os.mkdir('/content/drive/MyDrive/Image_dataset_super_resolution_unziped/reduced_dataset')\n",
        "os.mkdir('/content/drive/MyDrive/Image_dataset_super_resolution_unziped/reduced_dataset/reduced_dataset/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16-NOUvPODjx"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "directory = '/content/drive/MyDrive/Image_dataset_super_resolution_unziped/trail1/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4AZPwi6sSsL"
      },
      "outputs": [],
      "source": [
        "from keras.layers import LeakyReLU\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import PIL\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tqdm.notebook as tq\n",
        "import os\n",
        "from PIL import Image\n",
        "from tensorflow import keras\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import initializers\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.activations import relu\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import UpSampling2D\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "import time\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD6Q6RndQX3s"
      },
      "outputs": [],
      "source": [
        "#directory = '/content/drive/MyDrive/Image_dataset_super_resolution_unziped/img_align_celeba/img_align_celeba/'\n",
        "original_ht = 208\n",
        "original_width = 178\n",
        "diff = (original_ht - original_width)//2\n",
        "for image in tq.tqdm(os.listdir(directory)):\n",
        "    img = Image.open(directory + image)\n",
        "    img = img.crop((0, diff, original_width, original_ht-diff))\n",
        "    img.thumbnail((128, 128), Image.ANTIALIAS)\n",
        "    img.save(\"/content/drive/MyDrive/Image_dataset_super_resolution_unziped/reduced_dataset/reduced_dataset/\" + image)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Image_dataset_super_resolution_unziped/reduced_dataset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOjF4Qf37vsk",
        "outputId": "b88fd2d6-8266-4776-e632-74adccb3dd8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Image_dataset_super_resolution_unziped/reduced_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using ImageDataGenerator of Keras to load the large dataset into batches."
      ],
      "metadata": {
        "id": "QnB8LURRgXI1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUhpYkIRLcC5"
      },
      "outputs": [],
      "source": [
        "def preprocessing_function(x):\n",
        "    return x/128. - 1.\n",
        "\n",
        "datagen = ImageDataGenerator(preprocessing_function=preprocessing_function, validation_split=0.1)\n",
        "\n",
        "train_ds = datagen.flow_from_directory('/content/drive/MyDrive/Image_dataset_super_resolution_unziped/reduced_dataset/',\n",
        "                                             target_size=(128, 128), batch_size=20,\n",
        "                                             class_mode=None, subset='training')\n",
        "valid_ds = datagen.flow_from_directory('/content/drive/MyDrive/Image_dataset_super_resolution_unziped/reduced_dataset/',\n",
        "                                             target_size=(128, 128), batch_size=20,\n",
        "                                             class_mode=None, subset='validation')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Architecture"
      ],
      "metadata": {
        "id": "Erdhx-jpgQVw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqpRRwNZzSKP"
      },
      "outputs": [],
      "source": [
        "class ResidualUnit(keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, strides=1, activation=LeakyReLU(alpha=0.2), **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.main_layers = [\n",
        "                            Conv2D(filters, kernel_size=3, strides=strides, padding=\"SAME\"),\n",
        "                            BatchNormalization(),\n",
        "                            self.activation,\n",
        "                            Conv2D(filters, kernel_size=3, strides=strides, padding=\"SAME\"),\n",
        "                            BatchNormalization(),\n",
        "        ]\n",
        "        self.skip_layers = [\n",
        "                            Conv2D(filters, kernel_size=1, strides=strides, padding=\"SAME\"),\n",
        "                            BatchNormalization(),\n",
        "        ]\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.main_layers:\n",
        "            Z = layer(Z)\n",
        "        skip_Z = inputs\n",
        "        for layer in self.skip_layers:\n",
        "            skip_Z = layer(skip_Z)\n",
        "        return self.activation(Z + skip_Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cImfiOYOsehr"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR6gS7aksmRA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense, UpSampling2D\n",
        ")\n",
        "from tensorflow.keras import backend as K\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqxkb3WwzlXv"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "generator = keras.models.Sequential()                                                   \n",
        "generator.add(Conv2D(64, kernel_size=7, strides=1, padding=\"SAME\",\n",
        "           activation=LeakyReLU(alpha=0.2), input_shape=[32, 32, 3]))\n",
        "\n",
        "for filters in [256, 128, 64]:\n",
        "    generator.add(ResidualUnit(filters, kernel_size=3, strides=1))\n",
        "    generator.add(ResidualUnit(filters, kernel_size=3, strides=1))\n",
        "\n",
        "generator.add(UpSampling2D(size=2))\n",
        "generator.add(Conv2D(64, kernel_size=3, strides=1, padding=\"SAME\"))\n",
        "generator.add(BatchNormalization())\n",
        "Activation(LeakyReLU(alpha=0.2))\n",
        "\n",
        "generator.add(UpSampling2D(size=2))\n",
        "generator.add(Conv2D(3, kernel_size=9, strides=1, padding=\"SAME\",\n",
        "           activation='tanh'))\n",
        "\n",
        "\n",
        "discriminator = keras.models.Sequential()\n",
        "discriminator.add(Conv2D(64, kernel_size=3, strides=1, padding=\"SAME\",\n",
        "            activation=LeakyReLU(alpha=0.2), input_shape=[128, 128, 3]))\n",
        "\n",
        "for filters in [64, 128, 256, 512]:\n",
        "    discriminator.add(Conv2D(filters, kernel_size=3, strides=2, padding=\"SAME\"))\n",
        "    discriminator.add(BatchNormalization())\n",
        "    Activation(LeakyReLU(alpha=0.2))\n",
        "\n",
        "discriminator.add(Conv2D(256, kernel_size=3, strides=1, padding=\"SAME\"))\n",
        "discriminator.add(BatchNormalization())\n",
        "Activation(LeakyReLU(alpha=0.2))\n",
        "#from keras.layers import GlobalAveragePooling2D()\n",
        "discriminator.add(Dense(1024))\n",
        "discriminator.add(BatchNormalization())\n",
        "discriminator.add(keras.layers.GlobalAveragePooling2D())\n",
        "discriminator.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foKwir3u1HGo"
      },
      "outputs": [],
      "source": [
        "generator.summary()\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sf5BUjhD1OKb"
      },
      "outputs": [],
      "source": [
        "def plot_gan(SR_images, LR_images):\n",
        "    fig = plt.figure(figsize=(12,3))\n",
        "    for i in range(8):\n",
        "        plt.subplot(2, 8, i+1)\n",
        "        plt.imshow((SR_images[i] + 1 )*0.5)\n",
        "        plt.axis('off')\n",
        "        \n",
        "        plt.subplot(2, 8, i+8+1)\n",
        "        plt.imshow((LR_images[i] + 1 )*0.5)\n",
        "        plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYgvLs721Q9i"
      },
      "outputs": [],
      "source": [
        "def save_GIF_images(model, GIF_seed, epoch):\n",
        "    pred = model(GIF_seed, training = False)\n",
        "    fig = plt.figure(figsize=(12, 3))\n",
        "\n",
        "    for i in range(8):\n",
        "        plt.subplot(2, 8, i+1)\n",
        "        plt.imshow((pred[i] + 1 )*0.5)\n",
        "        plt.axis('off')\n",
        "    \n",
        "        plt.subplot(2, 8, i+8+1)\n",
        "        plt.imshow((GIF_seed[i] + 1 )*0.5)\n",
        "        plt.axis('off')\n",
        "    \n",
        "    plt.savefig('/content/drive/MyDrive/Colab/images_from_gan/image_at_epoch_{:04d}.png'.format(epoch + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVJZUE2D1TlI"
      },
      "outputs": [],
      "source": [
        "def mse_loss(HR_batch, SR_batch):\n",
        "    return tf.reduce_mean(tf.square(HR_batch - SR_batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6gnIx-A4T7c"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input\n",
        "from tensorflow.keras import optimizers\n",
        "import tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47oqKVek1Y1f"
      },
      "outputs": [],
      "source": [
        "discriminator_optimizer = tensorflow.keras.optimizers.RMSprop(lr=.0001, clipvalue=1.0, decay=1e-8)\n",
        "discriminator.compile(loss=\"binary_crossentropy\", optimizer=discriminator_optimizer)\n",
        "discriminator.trainable = False\n",
        "input = Input(shape=[32, 32, 3])\n",
        "SR_image = generator(input)\n",
        "gan_output = discriminator(SR_image)\n",
        "gan = Model(inputs=input, outputs=[SR_image, gan_output])\n",
        "generator_optimizer = tensorflow.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
        "gan.compile(loss=[mse_loss, \"binary_crossentropy\"], loss_weights=[0.8, 0.2],optimizer=generator_optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGKs7Gy31cFA"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = '/content/drive/MyDrive/Colab/Super Resolution using DCGAN/checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOz4LpQo6V9N"
      },
      "outputs": [],
      "source": [
        "def train_gan(generator, discriminator, dataset, batch_size, epochs):\n",
        "    GIF_seed = tf.image.resize(valid_ds[0], size=[32, 32])\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        G_loss_epoch = 0\n",
        "        D_loss_epoch = 0\n",
        "        print(\"Epoch no. \" + str(epoch + 1))\n",
        "        start_time = time.time()\n",
        "        \n",
        "        for batch_number in tq.tqdm(range(dataset.n//batch_size)):\n",
        "            HR_batch = dataset[batch_number]\n",
        "            LR_batch = tf.image.resize(HR_batch, size=[32, 32])\n",
        "            \n",
        "            #PHASE 1: train the Discriminator\n",
        "            SR_batch = generator(LR_batch)\n",
        "            random_wrong_labels = np.random.binomial(1, 0.05, size=[batch_size, 1])\n",
        "            \n",
        "            Y1 = tf.constant([[0.]]*batch_size)\n",
        "            Y1 += .1 * np.random.random_sample(Y1.shape)\n",
        "            discriminator.trainable = True\n",
        "            D_loss_epoch += discriminator.train_on_batch(SR_batch, Y1)\n",
        "            \n",
        "            Y1 = tf.constant([[1.]]*batch_size)\n",
        "            Y1 -= .1 * np.random.random_sample(Y1.shape)\n",
        "            discriminator.trainable = True\n",
        "            D_loss_epoch += discriminator.train_on_batch(HR_batch, Y1)\n",
        "                \n",
        "            \n",
        "            #PHASE 2: train the Generator\n",
        "            random_wrong_labels = np.random.binomial(1, 0.05, size=[batch_size, 1])\n",
        "            Y2 = tf.constant([[1.]]*batch_size)\n",
        "            Y2 -= .1 * np.random.random_sample(Y2.shape)\n",
        "            discriminator.trainable = False\n",
        "            G_losses = gan.train_on_batch(LR_batch, [HR_batch, Y2])\n",
        "            G_loss_epoch += G_losses[0]\n",
        "\n",
        "            if (batch_number + 1)%500 == 0:\n",
        "                plot_gan(SR_batch, LR_batch)\n",
        "                j = np.random.randint(0, 50)\n",
        "                plot_gan(generator(tf.image.resize(valid_ds[j], size=[32, 32])), tf.image.resize(valid_ds[j], size=[32, 32]))\n",
        "                checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "        D_loss_epoch /= (dataset.n//batch_size)\n",
        "        G_loss_epoch /= (dataset.n//batch_size)\n",
        "        print(\"D_loss = \"+str(D_loss_epoch)+\"   G_loss = \"+str(G_loss_epoch)+\"   @epoch \"+str(epoch+1)+\"   time = \"+str(time.time() - start_time))\n",
        "        plot_gan(SR_batch, LR_batch)\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"Saving weights and generating GIF images\")\n",
        "        save_GIF_images(generator, GIF_seed, epoch)\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "        \n",
        "    save_GIF_images(generator, GIF_seed, epoch)\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8DpESGn6Z-C"
      },
      "outputs": [],
      "source": [
        "train_gan(generator, discriminator, train_ds, 20, epochs=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results are in the accompanying Github README.md"
      ],
      "metadata": {
        "id": "M6BoJFf3giCJ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "GAN image_super_resolution.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}